{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "One of the pages I want to create in the Dash app is a \"transcription display\" page. The user will see a video at the top of their page, and will have the transcription below, with each of the segments colored according to how similar a particular segment is to a search query. \n",
    "\n",
    "This notebook will help facilitate the creation of a prototype of that page! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will help set up the rest of the notebook. \n",
    "\n",
    "I'm going to start by changing my working directory to the repo root. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\data\\programming\\neural-needle-drop\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll import different modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import pandas as pd\n",
    "import pinecone\n",
    "import os\n",
    "import mysql.connector\n",
    "import traceback\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from time import time\n",
    "import requests\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "import json\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, I'm going to set up a connection to my MySQL database. This will help me load in the relevant data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the connection to the MySQL server\n",
    "cnx = mysql.connector.connect(\n",
    "    user='root', password=os.getenv(\"MYSQL_PASSWORD\"), \n",
    "    host='localhost', database='neural-needle-drop')\n",
    "\n",
    "# Create a cursor \n",
    "cursor = cnx.cursor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're going to set up the connection to the Pinecone API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.2,\n",
       " 'namespaces': {'video_embeddings': {'vector_count': 130836}},\n",
       " 'total_vector_count': 130836}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Pinecone API connection\n",
    "pinecone.init(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "# Setting up the index \n",
    "pinecone_index = pinecone.Index(\"neural-needledrop-prototype\")\n",
    "pinecone_index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "The methods below will also help throughout the rest of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will return a list of ndarrays, each representing text embeddings of \n",
    "# the text in each index of the input_text_list list\n",
    "def generate_embeddings(input_text_list, print_exceptions=False):\n",
    "    \n",
    "    # Get the OpenAI API key from the environment variables \n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "    \n",
    "    # Build the API request\n",
    "    url = \"https://api.openai.com/v1/embeddings\"\n",
    "    headers = CaseInsensitiveDict()\n",
    "    headers[\"Content-Type\"] = \"application/json\"\n",
    "    headers[\"Authorization\"] = \"Bearer \" + api_key\n",
    "    data = \"\"\"{\"input\": \"\"\" + json.dumps(input_text_list) + \"\"\",\"model\":\"text-embedding-ada-002\"}\"\"\"\n",
    "    \n",
    "    # Send the API request\n",
    "    resp = requests.post(url, headers=headers, data=data)\n",
    "    \n",
    "    # If the request was successful, return ndarrays of the embeddings. Otherwise, return None objects \n",
    "    if resp.status_code == 200:\n",
    "        return [np.asarray(data_object['embedding']) for data_object in resp.json()['data']]\n",
    "    else:\n",
    "        if (print_exceptions):\n",
    "            print(resp.json())\n",
    "        return [None for txt in input_text_list]\n",
    "    \n",
    "# This method will generate the embedding for a single string\n",
    "def generate_embedding(txt_input, print_exceptions=False):\n",
    "    return (generate_embeddings([txt_input], print_exceptions)[0])\n",
    "\n",
    "def query_to_df(query, print_error=False):\n",
    "    '''Query the active MySQL database and return results in a DataFrame'''\n",
    "\n",
    "    # Try to return the results as a DataFrame\n",
    "    try:\n",
    "        # Execute the query\n",
    "        cursor.execute(query)\n",
    "\n",
    "        # Fetch the results \n",
    "        res = cursor.fetchall()\n",
    "\n",
    "        # Return a DataFrame\n",
    "        return pd.DataFrame(res, columns=[i[0] for i in cursor.description])\n",
    "\n",
    "    # If we run into an Exception, return None\n",
    "    except Exception as e:\n",
    "        if (print_error):\n",
    "            print(f\"Ran into the following error:\\n{e}\\nStack trace:\")\n",
    "            print(traceback.format_exc())\n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Data\n",
    "Now that everything is properly set up, we'll want to collect the data relevant to this page. \n",
    "\n",
    "Below, I'll indicate a handful of different things that'll be used as \"seeds\" to the data collection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the user's search query\n",
    "user_search_query = \"Man on the moon, spaceship going to the firey depths of Hell\"\n",
    "\n",
    "# This is the embedding of the user's search query \n",
    "user_search_query_emb = generate_embedding(user_search_query, print_exceptions=True)\n",
    "\n",
    "# This is the video that the user wants to compare their search query to \n",
    "cur_video_id = \"J_LvLhFq2IU\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aforementioned information will have already been generated by the user's behavior on the \"Search\" page; when they click on one of the videos, they'll be moved to this Transcription Display page. \n",
    "\n",
    "The cell below will collect the relevant data from Pinecone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the Pinecone index for all of the \n",
    "pinecone_results = pinecone_index.query(\n",
    "    vector=user_search_query_emb,\n",
    "    filter={\n",
    "        \"embedding_type\": \"segment_chunk\",\n",
    "        \"video_id\": cur_video_id\n",
    "    },\n",
    "    top_k=10000,\n",
    "    include_metadata=True,\n",
    "    namespace=\"video_embeddings\"\n",
    ")\n",
    "\n",
    "# Create a DataFrame from the Pinecone results \n",
    "top_segment_matches_original_df = pd.DataFrame.from_records(\n",
    "    [{\"id\": x.id, \"score\": x.score} | x.metadata\n",
    "        for x in pinecone_results['matches']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to collect the relevant data from the `transcriptions` table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_video_transcription_df = query_to_df(f\"\"\"SELECT * FROM transcriptions WHERE id=\"{cur_video_id}\" AND segment != -1\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're going to need to transform all of this data. There are two data structures that we want to create: \n",
    "\n",
    "- **Segment Chunks** - This is a list of the different \"segment chunks\" and their relevance to the search query. \n",
    "- **Segment Info** - This is essentially just the `cur_video_transcription_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the segment_chunk_df\n",
    "segment_chunk_df = top_segment_matches_original_df[[\"start_segment\", \"end_segment\", \"score\"]].copy()\n",
    "\n",
    "# Creating the segment_info_df\n",
    "segment_info_df = cur_video_transcription_df[[\"segment\", \"start_time\", \"text\"]].rename(\n",
    "    columns={\"start_time\": \"seek\"}).copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to save these DataFrames - they'll be the basis for testing out our page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our DataFrames\n",
    "segment_chunk_df.to_json(\"data/test_segment_chunk_df.json\", orient=\"records\", indent=2)\n",
    "segment_info_df.to_json(\"data/test_segment_info_df.json\", orient=\"records\", indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70b6d7546de29863a58b9c57a86bc7d85299cc84f1fdf81f78934173ede7c021"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
