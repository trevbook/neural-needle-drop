{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "I want to try and re-create the video search using MySQL and Pinecone. That way, I can quickly prototype the Dash UI. This notebook will contain code for that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will help set up the rest of the notebook. \n",
    "\n",
    "I'm going to start by changing my working directory to the repo root. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\data\\programming\\neural-needle-drop\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll import different modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import pandas as pd\n",
    "import pinecone\n",
    "import os\n",
    "import mysql.connector\n",
    "import traceback\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from time import time\n",
    "import requests\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "import json\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, I'm going to set up a connection to my MySQL database. This will help me load in the relevant data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the connection to the MySQL server\n",
    "cnx = mysql.connector.connect(\n",
    "    user='root', password=os.getenv(\"MYSQL_PASSWORD\"), \n",
    "    host='localhost', database='neural-needle-drop')\n",
    "\n",
    "# Create a cursor \n",
    "cursor = cnx.cursor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're going to set up the connection to the Pinecone API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.2,\n",
       " 'namespaces': {'video_embeddings': {'vector_count': 69496}},\n",
       " 'total_vector_count': 69496}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Pinecone API connection\n",
    "pinecone.init(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "# Setting up the index \n",
    "pinecone_index = pinecone.Index(\"neural-needledrop-prototype\")\n",
    "pinecone_index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "The methods below will also help throughout the rest of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will return a list of ndarrays, each representing text embeddings of \n",
    "# the text in each index of the input_text_list list\n",
    "def generate_embeddings(input_text_list, print_exceptions=False):\n",
    "    \n",
    "    # Get the OpenAI API key from the environment variables \n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "    \n",
    "    # Build the API request\n",
    "    url = \"https://api.openai.com/v1/embeddings\"\n",
    "    headers = CaseInsensitiveDict()\n",
    "    headers[\"Content-Type\"] = \"application/json\"\n",
    "    headers[\"Authorization\"] = \"Bearer \" + api_key\n",
    "    data = \"\"\"{\"input\": \"\"\" + json.dumps(input_text_list) + \"\"\",\"model\":\"text-embedding-ada-002\"}\"\"\"\n",
    "    \n",
    "    # Send the API request\n",
    "    resp = requests.post(url, headers=headers, data=data)\n",
    "    \n",
    "    # If the request was successful, return ndarrays of the embeddings. Otherwise, return None objects \n",
    "    if resp.status_code == 200:\n",
    "        return [np.asarray(data_object['embedding']) for data_object in resp.json()['data']]\n",
    "    else:\n",
    "        if (print_exceptions):\n",
    "            print(resp.json())\n",
    "        return [None for txt in input_text_list]\n",
    "    \n",
    "# This method will generate the embedding for a single string\n",
    "def generate_embedding(txt_input, print_exceptions=False):\n",
    "    return (generate_embeddings([txt_input], print_exceptions)[0])\n",
    "\n",
    "def query_to_df(query, print_error=False):\n",
    "    '''Query the active MySQL database and return results in a DataFrame'''\n",
    "\n",
    "    # Try to return the results as a DataFrame\n",
    "    try:\n",
    "        # Execute the query\n",
    "        cursor.execute(query)\n",
    "\n",
    "        # Fetch the results \n",
    "        res = cursor.fetchall()\n",
    "\n",
    "        # Return a DataFrame\n",
    "        return pd.DataFrame(res, columns=[i[0] for i in cursor.description])\n",
    "\n",
    "    # If we run into an Exception, return None\n",
    "    except Exception as e:\n",
    "        if (print_error):\n",
    "            print(f\"Ran into the following error:\\n{e}\\nStack trace:\")\n",
    "            print(traceback.format_exc())\n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Flow\n",
    "Below, I'm going to try and replicate the \"search flow\" - eventually, I'd wrap this up into a method of its own. \n",
    "\n",
    "1. Get the embedding for your search string from OpenAI\n",
    "2. Search Pinecone for the most similar embeddings\n",
    "3. Do some data transformation to determine which videos have the largest amount of similar segments\n",
    "4. Query the MySQL database in order to determine some of the information for that particular set of videos\n",
    "\n",
    "I'll kick off the first step: getting the embedding for my search string! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the user's search string\n",
    "search_str = \"You'll go to hell for what your dirty mind is thinking\"\n",
    "\n",
    "# Get the embedding for the search string\n",
    "search_str_emb = generate_embedding(search_str, print_exceptions=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll search Pinecone for the most similar embeddings. I'll try and only look for embeddings that're segment chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the Pinecone index for the 5000 most similar \n",
    "pinecone_results = pinecone_index.query(\n",
    "    vector=search_str_emb.tolist(),\n",
    "    filter={\n",
    "        \"embedding_type\": \"segment_chunk\"\n",
    "    },\n",
    "    top_k=5000,\n",
    "    include_metadata=True,\n",
    "    namespace=\"video_embeddings\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I should be able to parse these results into a DataFrame, and then run some manipulations on it in order to get a sense of which videos I ought to query for from MySQL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the Pinecone results \n",
    "top_segment_matches_original_df = pd.DataFrame.from_records(\n",
    "    [{\"id\": x.id, \"score\": x.score} | x.metadata\n",
    "        for x in pinecone_results['matches']])\n",
    "\n",
    "grouped_sorted_segment_df = top_segment_matches_original_df.groupby(\"video_id\")\n",
    "top_segment_matches_df = grouped_sorted_segment_df.apply(\n",
    "    lambda x: x.sort_values(\"score\", ascending=False).head(5)).reset_index(\n",
    "    drop=True).copy()\n",
    "\n",
    "# Determine the average score across the different videos \n",
    "avg_segment_sim_by_video_df = top_segment_matches_df.groupby(\"video_id\")[\"score\"].mean(numeric_only=True).reset_index().rename(\n",
    "    columns={\"score\": \"avg_segment_sim\"}).sort_values(\"avg_segment_sim\", ascending=False)\n",
    "\n",
    "median_segment_sim_by_video_df = top_segment_matches_df.groupby(\"video_id\")[\"score\"].median(numeric_only=True).reset_index().rename(\n",
    "    columns={\"score\": \"median_segment_sim\"}).sort_values(\"median_segment_sim\", ascending=False)\n",
    "\n",
    "segment_ct_by_video_df = top_segment_matches_df.groupby(\"video_id\").count().reset_index().rename(\n",
    "    columns={\"id\": \"segment_ct\"}).sort_values(\"segment_ct\", ascending=False)[[\"video_id\", \"segment_ct\"]]\n",
    "\n",
    "# Create the \"scored_video_df\", which tries to merge some degree of \"relevance\" and \"frequency\"\n",
    "scored_video_df = segment_ct_by_video_df.merge(avg_segment_sim_by_video_df, on=\"video_id\")\n",
    "scored_video_df = scored_video_df.merge(median_segment_sim_by_video_df, on=\"video_id\")\n",
    "scored_video_df[\"neural_search_score\"] = scored_video_df[\"segment_ct\"] * scored_video_df[\"avg_segment_sim\"]\n",
    "scored_video_df = scored_video_df.sort_values(\"neural_search_score\", ascending=False)\n",
    "\n",
    "# We'll also add in information about the most similar segment in each video \n",
    "top_single_segments_per_video_df = top_segment_matches_df[top_segment_matches_df[\"video_id\"].isin(\n",
    "    list(scored_video_df.head(10)[\"video_id\"]))]\n",
    "grouped_sorted_segment_df = top_single_segments_per_video_df.groupby(\"video_id\")\n",
    "top_single_segments_per_video_df = grouped_sorted_segment_df.apply(\n",
    "    lambda x: x.sort_values(\"score\", ascending=False).head(1)).reset_index(\n",
    "    drop=True).copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've got this \"scored neural search\" DataFrame, I'm going to grab the relevant video information from MySQL. I'll join this with the `scored_video_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query will determine the information for the top videos\n",
    "top_scored_video_info_query_filter_str = \" OR \".join([f'id=\"{row.video_id}\"' for row in scored_video_df.head(10).itertuples()])\n",
    "top_scored_video_info_query = f\"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    video_details\n",
    "WHERE {top_scored_video_info_query_filter_str}\"\"\"\n",
    "\n",
    "# Execute the above query \n",
    "top_scored_video_info_df = query_to_df(top_scored_video_info_query, print_error=True)\n",
    "\n",
    "# Merge in some of the scores \n",
    "top_scored_video_info_df = top_scored_video_info_df.merge(scored_video_df, left_on=\"id\", right_on=\"video_id\").drop(\n",
    "    columns=[\"video_id\"]).sort_values(\"neural_search_score\", ascending=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're going to query the MySQL database once more to get the text of the transcription segments corresponding to these embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a \"filter string\" for the transcription query\n",
    "all_video_filter_str_list = []\n",
    "for row in top_single_segments_per_video_df.itertuples():\n",
    "    segment_filter_str = \" OR \".join([f\"segment={num}\" for num in list(range(int(row.start_segment), int(row.end_segment)+1))])\n",
    "    all_video_filter_str_list.append(f\"id='{row.video_id}' AND ({segment_filter_str})\")\n",
    "transcription_filter_str = \" OR \".join([f\"({cur_vid_filter_str})\" for cur_vid_filter_str in all_video_filter_str_list])\n",
    "\n",
    "# Crafting the transcription query \n",
    "top_segment_transcriptions_query = f\"\"\"SELECT * FROM transcriptions WHERE {transcription_filter_str}\"\"\"\n",
    "\n",
    "# Executing the transcription query \n",
    "top_segment_transcriptions_df = query_to_df(top_segment_transcriptions_query, print_error=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're going to merge these \"top segments\" back into the `top_scored_video_info_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join together the individual segments to create segment chunks\n",
    "top_segment_chunk_per_video_df = top_segment_transcriptions_df.groupby(\"id\")[\"text\"].apply(list).reset_index()\n",
    "top_segment_chunk_per_video_df[\"text\"] = top_segment_chunk_per_video_df[\"text\"].apply(\n",
    "    lambda seg_list: \" \".join([seg.strip() for seg in seg_list]))\n",
    "top_segment_chunk_per_video_df = top_segment_chunk_per_video_df.rename(columns={\"text\": \"segment_transcription\"})\n",
    "top_segment_chunk_per_video_df = top_segment_chunk_per_video_df.merge(\n",
    "    top_single_segments_per_video_df[[\"score\", \"video_id\"]].rename({\"score\": \"top_segment_score\"}), \n",
    "    left_on=\"id\", right_on=\"video_id\")\n",
    "\n",
    "# Merge these segment chunks back into the top_scored_video_info_df DataFrame\n",
    "top_scored_video_info_df = top_scored_video_info_df.merge(top_segment_chunk_per_video_df, on=\"id\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodized Search\n",
    "Now that I've got a search method sketched out, I can turn it into a method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_tnd_video_search(search_str):\n",
    "\n",
    "    \"\"\"\n",
    "    This method will run 'neural search' across all of TheNeedleDrop's videos, using \n",
    "    the methodology I'd established with Pinecone and MySQL. \n",
    "\n",
    "    It'll return two DataFrames: one containing the information about the top scoring \n",
    "    videos, and another containing the segments that scored the highest for this video.  \n",
    "    \"\"\"\n",
    "\n",
    "    # Get the embedding for the search string\n",
    "    search_str_emb = generate_embedding(search_str)\n",
    "\n",
    "    # ================================================\n",
    "\n",
    "    # Query the Pinecone index for the 5000 most similar \n",
    "    pinecone_results = pinecone_index.query(\n",
    "        vector=search_str_emb.tolist(),\n",
    "        filter={\n",
    "            \"embedding_type\": \"segment_chunk\"\n",
    "        },\n",
    "        top_k=3000,\n",
    "        include_metadata=True,\n",
    "        namespace=\"video_embeddings\"\n",
    "    )\n",
    "\n",
    "    # ================================================\n",
    "\n",
    "    # Create a DataFrame from the Pinecone results \n",
    "    top_segment_matches_original_df = pd.DataFrame.from_records(\n",
    "        [{\"id\": x.id, \"score\": x.score} | x.metadata\n",
    "            for x in pinecone_results['matches']])\n",
    "\n",
    "    grouped_sorted_segment_df = top_segment_matches_original_df.groupby(\"video_id\")\n",
    "    top_segment_matches_df = grouped_sorted_segment_df.apply(\n",
    "        lambda x: x.sort_values(\"score\", ascending=False).head(5)).reset_index(\n",
    "        drop=True).copy()\n",
    "\n",
    "    # Determine the average score across the different videos \n",
    "    avg_segment_sim_by_video_df = top_segment_matches_df.groupby(\"video_id\")[\"score\"].mean(numeric_only=True).reset_index().rename(\n",
    "        columns={\"score\": \"avg_segment_sim\"}).sort_values(\"avg_segment_sim\", ascending=False)\n",
    "\n",
    "    median_segment_sim_by_video_df = top_segment_matches_df.groupby(\"video_id\")[\"score\"].median(numeric_only=True).reset_index().rename(\n",
    "        columns={\"score\": \"median_segment_sim\"}).sort_values(\"median_segment_sim\", ascending=False)\n",
    "\n",
    "    segment_ct_by_video_df = top_segment_matches_df.groupby(\"video_id\").count().reset_index().rename(\n",
    "        columns={\"id\": \"segment_ct\"}).sort_values(\"segment_ct\", ascending=False)[[\"video_id\", \"segment_ct\"]]\n",
    "\n",
    "    # Create the \"scored_video_df\", which tries to merge some degree of \"relevance\" and \"frequency\"\n",
    "    scored_video_df = segment_ct_by_video_df.merge(avg_segment_sim_by_video_df, on=\"video_id\")\n",
    "    scored_video_df = scored_video_df.merge(median_segment_sim_by_video_df, on=\"video_id\")\n",
    "    scored_video_df[\"neural_search_score\"] = scored_video_df[\"segment_ct\"] * scored_video_df[\"avg_segment_sim\"]\n",
    "    scored_video_df = scored_video_df.sort_values(\"neural_search_score\", ascending=False)\n",
    "\n",
    "    # We'll also add in information about the most similar segment in each video \n",
    "    top_single_segments_per_video_df = top_segment_matches_df[top_segment_matches_df[\"video_id\"].isin(\n",
    "        list(scored_video_df.head(10)[\"video_id\"]))]\n",
    "    grouped_sorted_segment_df = top_single_segments_per_video_df.groupby(\"video_id\")\n",
    "    top_single_segments_per_video_df = grouped_sorted_segment_df.apply(\n",
    "        lambda x: x.sort_values(\"score\", ascending=False).head(1)).reset_index(\n",
    "        drop=True).copy()\n",
    "\n",
    "    # ================================================\n",
    "\n",
    "    # This query will determine the information for the top videos\n",
    "    top_scored_video_info_query_filter_str = \" OR \".join([f'id=\"{row.video_id}\"' for row in scored_video_df.head(10).itertuples()])\n",
    "    top_scored_video_info_query = f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM\n",
    "        video_details\n",
    "    WHERE {top_scored_video_info_query_filter_str}\"\"\"\n",
    "\n",
    "    # Execute the above query \n",
    "    top_scored_video_info_df = query_to_df(top_scored_video_info_query, print_error=True)\n",
    "\n",
    "    # Merge in some of the scores \n",
    "    top_scored_video_info_df = top_scored_video_info_df.merge(scored_video_df, left_on=\"id\", right_on=\"video_id\").drop(\n",
    "        columns=[\"video_id\"]).sort_values(\"neural_search_score\", ascending=False)\n",
    "\n",
    "    # ================================================\n",
    "\n",
    "    # Creating a \"filter string\" for the transcription query\n",
    "    all_video_filter_str_list = []\n",
    "    for row in top_single_segments_per_video_df.itertuples():\n",
    "        segment_filter_str = \" OR \".join([f\"segment={num}\" for num in list(range(int(row.start_segment), int(row.end_segment)+1))])\n",
    "        all_video_filter_str_list.append(f\"id='{row.video_id}' AND ({segment_filter_str})\")\n",
    "    transcription_filter_str = \" OR \".join([f\"({cur_vid_filter_str})\" for cur_vid_filter_str in all_video_filter_str_list])\n",
    "\n",
    "    # Crafting the transcription query \n",
    "    top_segment_transcriptions_query = f\"\"\"SELECT * FROM transcriptions WHERE {transcription_filter_str}\"\"\"\n",
    "\n",
    "    # Executing the transcription query \n",
    "    top_segment_transcriptions_df = query_to_df(top_segment_transcriptions_query, print_error=True)\n",
    "\n",
    "    # ================================================\n",
    "\n",
    "    # Join together the individual segments to create segment chunks\n",
    "    top_segment_chunk_per_video_df = top_segment_transcriptions_df.groupby(\"id\")[\"text\"].apply(list).reset_index()\n",
    "    top_segment_chunk_per_video_df[\"text\"] = top_segment_chunk_per_video_df[\"text\"].apply(\n",
    "        lambda seg_list: \" \".join([seg.strip() for seg in seg_list]))\n",
    "    top_segment_chunk_per_video_df = top_segment_chunk_per_video_df.rename(columns={\"text\": \"segment_transcription\"})\n",
    "    top_segment_chunk_per_video_df = top_segment_chunk_per_video_df.merge(\n",
    "        top_single_segments_per_video_df[[\"score\", \"video_id\"]].rename(columns={\"score\": \"top_segment_score\"}), \n",
    "        left_on=\"id\", right_on=\"video_id\")\n",
    "\n",
    "    # Merge these segment chunks back into the top_scored_video_info_df DataFrame\n",
    "    top_scored_video_info_df = top_scored_video_info_df.merge(top_segment_chunk_per_video_df, on=\"id\")\n",
    "\n",
    "    # ================================================\n",
    "\n",
    "    return top_scored_video_info_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this search method in hand, I want to return the results! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>length</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>description</th>\n",
       "      <th>view_ct</th>\n",
       "      <th>channel_name</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>url</th>\n",
       "      <th>segment_ct</th>\n",
       "      <th>avg_segment_sim</th>\n",
       "      <th>median_segment_sim</th>\n",
       "      <th>neural_search_score</th>\n",
       "      <th>segment_transcription</th>\n",
       "      <th>top_segment_score</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q_gBZFYXHQI</td>\n",
       "      <td>Sun Kil Moon - \"War On Drugs: Suck My Cock\" TR...</td>\n",
       "      <td>487</td>\n",
       "      <td>UCt7fwAhXDy3oNFTAzF2o8Pw</td>\n",
       "      <td>Listen: http://sunkilmoon.com/mkwod/index.html...</td>\n",
       "      <td>73526</td>\n",
       "      <td>theneedledrop</td>\n",
       "      <td>2014-10-11</td>\n",
       "      <td>https://www.youtube.com/watch?v=q_gBZFYXHQI</td>\n",
       "      <td>5</td>\n",
       "      <td>0.806858</td>\n",
       "      <td>0.802795</td>\n",
       "      <td>4.034290</td>\n",
       "      <td>who he categorized as hillbillies to shut the ...</td>\n",
       "      <td>0.823755</td>\n",
       "      <td>q_gBZFYXHQI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kh2_xvw3LUA</td>\n",
       "      <td>David Byrne - American Utopia ALBUM REVIEW</td>\n",
       "      <td>418</td>\n",
       "      <td>UCt7fwAhXDy3oNFTAzF2o8Pw</td>\n",
       "      <td>Listen: https://www.youtube.com/watch?v=euEgyX...</td>\n",
       "      <td>96418</td>\n",
       "      <td>theneedledrop</td>\n",
       "      <td>2018-03-15</td>\n",
       "      <td>https://www.youtube.com/watch?v=Kh2_xvw3LUA</td>\n",
       "      <td>5</td>\n",
       "      <td>0.806043</td>\n",
       "      <td>0.801537</td>\n",
       "      <td>4.030215</td>\n",
       "      <td>American Utopia. David Byrne, one of the most ...</td>\n",
       "      <td>0.825939</td>\n",
       "      <td>Kh2_xvw3LUA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KwTAPEe6A1A</td>\n",
       "      <td>Lil B- I'm Gay (I'm Happy) ALBUM REVIEW</td>\n",
       "      <td>1223</td>\n",
       "      <td>UCt7fwAhXDy3oNFTAzF2o8Pw</td>\n",
       "      <td>Listen: http://www.youtube.com/watch?v=PcSfimb...</td>\n",
       "      <td>692780</td>\n",
       "      <td>theneedledrop</td>\n",
       "      <td>2011-07-04</td>\n",
       "      <td>https://www.youtube.com/watch?v=KwTAPEe6A1A</td>\n",
       "      <td>5</td>\n",
       "      <td>0.800715</td>\n",
       "      <td>0.800591</td>\n",
       "      <td>4.003576</td>\n",
       "      <td>and other controversies about him, you know, m...</td>\n",
       "      <td>0.814706</td>\n",
       "      <td>KwTAPEe6A1A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  length  \\\n",
       "0  q_gBZFYXHQI  Sun Kil Moon - \"War On Drugs: Suck My Cock\" TR...     487   \n",
       "1  Kh2_xvw3LUA         David Byrne - American Utopia ALBUM REVIEW     418   \n",
       "2  KwTAPEe6A1A            Lil B- I'm Gay (I'm Happy) ALBUM REVIEW    1223   \n",
       "\n",
       "                 channel_id  \\\n",
       "0  UCt7fwAhXDy3oNFTAzF2o8Pw   \n",
       "1  UCt7fwAhXDy3oNFTAzF2o8Pw   \n",
       "2  UCt7fwAhXDy3oNFTAzF2o8Pw   \n",
       "\n",
       "                                         description  view_ct   channel_name  \\\n",
       "0  Listen: http://sunkilmoon.com/mkwod/index.html...    73526  theneedledrop   \n",
       "1  Listen: https://www.youtube.com/watch?v=euEgyX...    96418  theneedledrop   \n",
       "2  Listen: http://www.youtube.com/watch?v=PcSfimb...   692780  theneedledrop   \n",
       "\n",
       "  publish_date                                          url  segment_ct  \\\n",
       "0   2014-10-11  https://www.youtube.com/watch?v=q_gBZFYXHQI           5   \n",
       "1   2018-03-15  https://www.youtube.com/watch?v=Kh2_xvw3LUA           5   \n",
       "2   2011-07-04  https://www.youtube.com/watch?v=KwTAPEe6A1A           5   \n",
       "\n",
       "   avg_segment_sim  median_segment_sim  neural_search_score  \\\n",
       "0         0.806858            0.802795             4.034290   \n",
       "1         0.806043            0.801537             4.030215   \n",
       "2         0.800715            0.800591             4.003576   \n",
       "\n",
       "                               segment_transcription  top_segment_score  \\\n",
       "0  who he categorized as hillbillies to shut the ...           0.823755   \n",
       "1  American Utopia. David Byrne, one of the most ...           0.825939   \n",
       "2  and other controversies about him, you know, m...           0.814706   \n",
       "\n",
       "      video_id  \n",
       "0  q_gBZFYXHQI  \n",
       "1  Kh2_xvw3LUA  \n",
       "2  KwTAPEe6A1A  "
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_scored_video_info_df = neural_tnd_video_search(\n",
    "    \"Bo Burnham Peace Hippy .\")\n",
    "\n",
    "top_scored_video_info_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm also going to try my hand at printing some stuff out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "    Sun Kil Moon - \"War On Drugs: Suck My Cock\" TRACK REVIEW ft. Very Naughty NSFW Words\n",
       "\n",
       "    who he categorized as hillbillies to shut the fuck up. But the biggest and most controversial incident of this occurred at an Ottawa folk festival where apparently Mark was playing at the same time as The War on Drugs, a Americana alt country psychedelic, Crouch Rock Band, whose album I reviewed earlier this year,\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "    David Byrne - American Utopia ALBUM REVIEW\n",
       "\n",
       "    American Utopia. David Byrne, one of the most enigmatic musical figures of the 1970s and 80s, is back with a new full-length album his first solo record in like 14 years or so.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "    Lil B- I'm Gay (I'm Happy) ALBUM REVIEW\n",
       "\n",
       "    and other controversies about him, you know, making tracks like, I'm God, that's small potatoes at this point. This album finds little Bee at the peak of his newest dilemma, naming his new album, I'm gay, I'm happy.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for row in top_scored_video_info_df.head(3).itertuples():\n",
    "    markdown_str = f\"\"\"\n",
    "    {row.title}\n",
    "\n",
    "    {row.segment_transcription}\n",
    "    \"\"\"\n",
    "\n",
    "    display(Markdown(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70b6d7546de29863a58b9c57a86bc7d85299cc84f1fdf81f78934173ede7c021"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
