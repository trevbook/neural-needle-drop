{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e09979fb-a35d-4503-8e47-55a34cd1f747",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "I wanted to play around with [OpenAI's Whisper](https://github.com/openai/whisper), so I've created this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666fac1-9152-43b8-9327-0f98eca128ea",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will help to set up the rest of the notebook.\n",
    "\n",
    "I'll start by changing directories to the root of the repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2f7051b-9b2b-4d09-9230-688aff2894a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Data\\Personal Study\\Programming\\neural-needle-drop\n"
     ]
    }
   ],
   "source": [
    "# Change the directory to the root of the repo\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5340028f-948f-4e44-96da-c2dd59bac1a8",
   "metadata": {},
   "source": [
    "Next, I'll import a couple of different libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99f15d61-912e-4bc5-8a4a-0dbfe612471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import subprocess\n",
    "import whisper\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import torch\n",
    "from Levenshtein import ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98115eb3-8cdf-43b7-ad87-0fa29f9bd8a8",
   "metadata": {},
   "source": [
    "Finally, I'll set up Whisper by loading the model. Since I have an 8GB GPU, I should be able to load in [their medium.en model](https://github.com/openai/whisper#available-models-and-languages). I'm using the English only model, since TheNeedleDrop reviews are in English. Apparently this model performs a little better for English only applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "231df06f-44bf-466c-8938-4f3296c37bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the 'medium.en' model into cuda in 7.60 seconds\n"
     ]
    }
   ],
   "source": [
    "# Determining whether we'll use GPU or CPU\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load in the model\n",
    "model_type = \"medium.en\"\n",
    "start_time=time()\n",
    "model = whisper.load_model(model_type, device=DEVICE)\n",
    "print(f\"Loaded the '{model_type}' model into {DEVICE} in {time()-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "053efc73-7052-4450-9e04-b1ab8f454042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 72.1M/72.1M [00:03<00:00, 23.0MiB/s]\n"
     ]
    }
   ],
   "source": [
    "# I also want to load in the base.en and tiny.en models - this way, I can compare their transcriptions\n",
    "base_model = whisper.load_model(\"base.en\", device=DEVICE)\n",
    "tiny_model = whisper.load_model(\"tiny.en\", device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854def84-87cb-4401-9958-56aff7f33410",
   "metadata": {},
   "source": [
    "# Experimentation\n",
    "Now that I've got the model loaded in, I want to test things out. I'm going to follow their [Python usage](https://github.com/openai/whisper#python-usage) quickstart within the repo's README. \n",
    "\n",
    "First question: how long does it take me to transcribe a single Anthony Fantano review? (The next couple of cells assume that you've run the **Experiments** Section of the **`Pytubes Experiments`** notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d5c09d1-bbef-4a97-bfca-95181ef77fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription took 9.3e+01 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Declare the path to the test file\n",
    "test_file_path = Path(\"data/test_audio_download.mp3\")\n",
    "\n",
    "# Time how long the transcription takes\n",
    "start_time = time()\n",
    "transcription = model.transcribe(str(test_file_path))\n",
    "print(f\"Transcription took {time()-start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ce9b88-2ca9-46d8-933d-ae75d9ffb6f9",
   "metadata": {},
   "source": [
    "Next: how *good* is the transcription? I'm going to paste the first couple hundred characters of it to understand what's going on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e011d45a-71f1-43c7-9b06-d4c32fcb535d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Yeah, yeah, hi everyone orange man almost gone here the Internet's busiest music nerd And it's time for a review of the new one Oh tricks point never album Magic one Oh tricks point never this is the latest LP from prolific composer producer sonic alchemist Daniel Lopatin aka one Oh tricks point never This I believe is his fourth full-length LP with the legendary Warp records and the project seems like a pretty huge concept for him an artistic self-portrait of sorts Maybe the records title flow and various interludes are all deeply inspired by the world of radio pretty interesting We're finally getting a deep dive on something like this especially considering the one Oh tricks point never name is a play on a radio station's frequency numbers anyway now when I understood this was the conceptual direction of this new album that excited me because personally I do have a lot of Passion for the world of radio ever since really I had a little boom box that I could tape my favorite songs off\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription[\"text\"][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea1be5-ab4e-44ce-b742-a104c953da18",
   "metadata": {},
   "source": [
    "Wow. It's almost perfect. This is about the first minute of the review. \n",
    "\n",
    "Let's transcribe it again, but this time using the base model. I want to compare the accuracy between the transcriptions. I'll use [python-Levenshtein](https://pypi.org/project/python-Levenshtein/) to determine how different the transcriptions are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9eb60d5d-b432-4f1b-93b3-9957ecb469d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription took 27.84 seconds.\n",
      "The Levenshtein ratio between the two transcriptions is 0.9653 (where 1 would be 'exactly equal')\n"
     ]
    }
   ],
   "source": [
    "# Time how long the transcription takes with the base model\n",
    "start_time = time()\n",
    "base_transcription = base_model.transcribe(str(test_file_path))\n",
    "print(f\"Transcription took {time()-start_time:.2f} seconds.\")\n",
    "\n",
    "# Calculate the Levensthein ratio between transcriptions\n",
    "lev_ratio = ratio(base_transcription['text'], transcription['text'])\n",
    "print(f\"The Levenshtein ratio between the two transcriptions is {lev_ratio:.4f} (where 1 would be 'exactly equal')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ddcfd7-36cd-4459-b5e1-4830fdc8ac14",
   "metadata": {},
   "source": [
    "So: for about a 200% speed increase, we still maintain a very similar transcription. Here's a look at the same amount of characters from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e622da7-a7ed-49f5-b3ae-2de66c40b3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Eee, yeah, yeah. Hi everyone, Orange Man almost gone here, the Internet's busiest music nerd, and it's time for a review of the new One O Tricks Point Never album, Magic One O Tricks Point Never. This is the latest LP from prolific composer, producer, sonic alchemist Daniel Lopez, AKA One O Tricks Point Never. This I believe is his fourth full-length LP with the legendary Warp Records, and the project seems like a pretty huge concept for him. An artistic self-portrait of sorts, maybe? The records, title, flow, and various interludes are all deeply inspired by the world of radio. Pretty interesting we're finally getting a deep dive on something like this, especially considering the One O Tricks Point Never name is a play on a radio station's frequency numbers anyway. Now, when I understood this was the conceptual direction of this new album, that excited me, because personally I do have a lot of passion for the world of radio. Ever since really I had a little boom box that I could tape\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_transcription[\"text\"][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8882342e-f68e-4e52-b621-4763bb1e5218",
   "metadata": {},
   "source": [
    "Finally, for one more comparison point: how does the tiny model differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd7dc887-9a06-4905-bff9-2ffd53f1d8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription took 19.12 seconds.\n",
      "The Levenshtein ratio between the two transcriptions is 0.9660 (where 1 would be 'exactly equal')\n"
     ]
    }
   ],
   "source": [
    "# Time how long the transcription takes with the base model\n",
    "start_time = time()\n",
    "tiny_transcription = tiny_model.transcribe(str(test_file_path))\n",
    "print(f\"Transcription took {time()-start_time:.2f} seconds.\")\n",
    "\n",
    "# Calculate the Levensthein ratio between transcriptions\n",
    "lev_ratio = ratio(tiny_transcription['text'], transcription['text'])\n",
    "print(f\"The Levenshtein ratio between the two transcriptions is {lev_ratio:.4f} (where 1 would be 'exactly equal')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37da893-ab57-4d0e-a093-e5d3d66903fb",
   "metadata": {},
   "source": [
    "And finally, a quick look at the \"tiny\" transcription:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d02b048-0e12-41ee-979a-b78c49de79e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Eeey, yah. Hi everyone, Orange Man almost gone here, the internet's busiest music nerd, and it's time for a review of the new Oneo Trix Point Never album, Magic Oneo Trix Point Never. This is the latest LP from prolific composer, producer, Sonic Alchemist, Daniel Lopeton, AKA Oneo Trix Point Never. This I believe is his fourth full-length LP with the legendary warp records, and the project seems like a pretty huge concept for him in artistic self-portrait of sorts, maybe? The records title flow in various interludes are all deeply inspired by the world of radio. Pretty interesting, we're finally getting a deep dive on something like this, especially considering the Oneo Trix Point Never name is a play on a radio station's frequency numbers anyway. Now, when I understood this was the conceptual direction of this new album, that excited me because personally I do have a lot of passion for the world of radio. Ever since really I had a little boom box that I could tape my favorite songs o\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_transcription[\"text\"][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5748e4a0-6ed2-43bc-aae8-db3bdd25f8a0",
   "metadata": {},
   "source": [
    "Seems really decent. This one (as well as the previous `base.en` transcription) seem to have better punctuation than the `medium.en` model, interestingly enough. Some of the capitalization in the `medium.en` is good, but I'm assumedly going to be normalizing capitalization for the embeddings regardless.\n",
    "\n",
    "I'll go with the tiny model for now. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
