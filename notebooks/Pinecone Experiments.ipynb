{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "I want to check out Pinecone, which is a vector database that I can use for semantic search. This notebook will contain a couple of my experiments with Pinecone. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will help set up the rest of the notebook. \n",
    "\n",
    "I'm going to start by changing my working directory to the repo root. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\data\\programming\\neural-needle-drop\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll import different modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import pandas as pd\n",
    "import pinecone\n",
    "import os\n",
    "import mysql.connector\n",
    "import traceback\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from time import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, I'm going to set up a connection to my MySQL database. This will help me load in the relevant data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the connection to the MySQL server\n",
    "cnx = mysql.connector.connect(\n",
    "    user='root', password=os.getenv(\"MYSQL_PASSWORD\"), \n",
    "    host='localhost', database='neural-needle-drop')\n",
    "\n",
    "# Create a cursor \n",
    "cursor = cnx.cursor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're going to set up the connection to the Pinecone API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Pinecone API connection\n",
    "pinecone.init(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "# Setting up the index \n",
    "pinecone_index = pinecone.Index(\"neural-needledrop-prototype\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "The cells below will define a couple of methods that'll be important for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_df(query, print_error=False):\n",
    "    '''Query the active MySQL database and return results in a DataFrame'''\n",
    "\n",
    "    # Try to return the results as a DataFrame\n",
    "    try:\n",
    "        # Execute the query\n",
    "        cursor.execute(query)\n",
    "\n",
    "        # Fetch the results \n",
    "        res = cursor.fetchall()\n",
    "\n",
    "        # Return a DataFrame\n",
    "        return pd.DataFrame(res, columns=[i[0] for i in cursor.description])\n",
    "\n",
    "    # If we run into an Exception, return None\n",
    "    except Exception as e:\n",
    "        if (print_error):\n",
    "            print(f\"Ran into the following error:\\n{e}\\nStack trace:\")\n",
    "            print(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "def chunks(iterable, batch_size=100):\n",
    "    \"\"\"A helper function to break an iterable into chunks of size batch_size.\"\"\"\n",
    "    it = iter(iterable)\n",
    "    chunk = tuple(itertools.islice(it, batch_size))\n",
    "    while chunk:\n",
    "        yield chunk\n",
    "        chunk = tuple(itertools.islice(it, batch_size))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Data\n",
    "In order to start working with the Pinecone index, I wanted to upload some data. I'll start with some of the whole-video embeddings. In order to upload those, I'll need to load them in from the SQL server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query will grab the entire embeddings table\n",
    "embeddings_df_query = \"\"\"SELECT * FROM embeddings\"\"\"\n",
    "\n",
    "# Execute the above query \n",
    "embeddings_df = query_to_df(embeddings_df_query, print_error=True)\n",
    "\n",
    "# Now, convert the embedding column from binary data to a list  \n",
    "embeddings_df[\"embedding\"] = embeddings_df[\"embedding\"].apply(lambda x: np.frombuffer(x).tolist())\n",
    "\n",
    "# Create the whole_video_embeddings_df by filtering the embeddings_df\n",
    "whole_video_embeddings_df = embeddings_df.query(\"embedding_type=='whole_video'\").copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm also going to load in all of the video details, just for effiency's sake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all of the data from the video_details table\n",
    "tnd_data_df = query_to_df(\n",
    "    \"\"\"SELECT * FROM video_details\"\"\", \n",
    "    print_error=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data loaded, I can start uploading information to Pinecone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [02:19<00:00,  3.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# We're going to break this DataFrame into chunks of 100\n",
    "chunk_size = 100\n",
    "chunk_amt = int(math.ceil(len(whole_video_embeddings_df)/chunk_size))\n",
    "for cur_chunk_num in tqdm(list(range(chunk_amt))):\n",
    "\n",
    "    # Subset the DataFrame so that we're only looking at <= 100 videos\n",
    "    df_chunk = whole_video_embeddings_df[cur_chunk_num*chunk_size:(cur_chunk_num+1)*chunk_size].copy()\n",
    "\n",
    "    # Format the data from this df_chunk into something you can send to Pinecone\n",
    "    pinecone_upsert_list = [(row.id, row.embedding, \n",
    "    {\"embedding_type\": row.embedding_type,\n",
    "     \"start_segment\": row.start_segment,\n",
    "     \"end_segment\": row.end_segment}) for row in df_chunk.itertuples()]\n",
    "    \n",
    "    # Upload this chunk to Pinecone\n",
    "    pinecone_index.upsert(vectors=pinecone_upsert_list, namespace=\"video_embeddings\")\n",
    "\n",
    "    # Sleep a little bit to avoid overwhelming Pinecone\n",
    "    sleep(1.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying Data\n",
    "Next, I want to try and query some data using Pinecone. They've got [some documentation on querying here](https://docs.pinecone.io/docs/query-data) - the cells below will try and replicate that.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That query took 0.11 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Extract the vector we want to search\n",
    "to_search_video_id = \"QaMpiKZh1fc\"\n",
    "to_search_embedding = whole_video_embeddings_df.query(\n",
    "    f\"id=='{to_search_video_id}'\").iloc[0].embedding\n",
    "\n",
    "# Querying Pinecone \n",
    "start_time = time()\n",
    "pinecone_query_results = pinecone_index.query(\n",
    "    vector=to_search_embedding,\n",
    "    top_k=5,\n",
    "    namespace=\"video_embeddings\")\n",
    "\n",
    "# Print some information about how long the query took\n",
    "print(f\"That query took {time()-start_time:.2f} seconds.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty fast, huh? Let's join the IDs together with their titles to figure out which videos are the most similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radiohead - Kid A ALBUM REVIEW\n",
      "SIMILARITY: 1.0\n",
      "\n",
      "Radiohead - A Moon Shaped Pool ALBUM REVIEW\n",
      "SIMILARITY: 0.897760153\n",
      "\n",
      "Radiohead: Worst To Best\n",
      "SIMILARITY: 0.897391081\n",
      "\n",
      "Radiohead - Burn the Witch TRACK REVIEW\n",
      "SIMILARITY: 0.893643439\n",
      "\n",
      "Talking Heads - Remain In Light ALBUM REVIEW\n",
      "SIMILARITY: 0.883516192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for match_dict in pinecone_query_results['matches']:\n",
    "    cur_match_video_details = tnd_data_df.query(f\"id=='{match_dict.id}'\").iloc[0]\n",
    "    print(f\"{cur_match_video_details.title}\\nSIMILARITY: {match_dict.score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70b6d7546de29863a58b9c57a86bc7d85299cc84f1fdf81f78934173ede7c021"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
