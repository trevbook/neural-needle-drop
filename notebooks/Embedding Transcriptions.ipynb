{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e845a9bf-f18f-47ff-ba1f-8d0f41957873",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "I wanted to experiment with using the OpenAI embedding endpoint to create embeddings for all of Fantano's videos! This notebook will establish a workflow for creating those embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e3b7d6-7908-4035-968a-0ccfe8720dee",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will help to set up the rest of the notebook. \n",
    "\n",
    "I'll start by changing my working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c5c66c0-a7aa-48c5-be33-99e853616e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Data\\Personal Study\\Programming\\neural-needle-drop\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1192d111-93d6-4188-95c2-6445a891d934",
   "metadata": {},
   "source": [
    "Now, I'll import some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab49bd45-52cb-4fbd-809e-fc5001279517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import requests\n",
    "import os\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "import numpy as np\n",
    "import json\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import math\n",
    "import traceback\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435999d-c502-43d6-9937-ac1fcf6f81bf",
   "metadata": {},
   "source": [
    "# Methods\n",
    "I've copied the methods below from the **OpenAI Embedding Experiment** notebook! They're going to help with getting the embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bc92fd2-7451-44f5-97cf-ba125b856e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will return a list of ndarrays, each representing text embeddings of \n",
    "# the text in each index of the input_text_list list\n",
    "def generate_embeddings(input_text_list, print_exceptions=False):\n",
    "    \n",
    "    # Get the OpenAI API key from the environment variables \n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "    \n",
    "    # Build the API request\n",
    "    url = \"https://api.openai.com/v1/embeddings\"\n",
    "    headers = CaseInsensitiveDict()\n",
    "    headers[\"Content-Type\"] = \"application/json\"\n",
    "    headers[\"Authorization\"] = \"Bearer \" + api_key\n",
    "    data = \"\"\"{\"input\": \"\"\" + json.dumps(input_text_list) + \"\"\",\"model\":\"text-embedding-ada-002\"}\"\"\"\n",
    "    \n",
    "    # Send the API request\n",
    "    resp = requests.post(url, headers=headers, data=data)\n",
    "    \n",
    "    # If the request was successful, return ndarrays of the embeddings. Otherwise, return None objects \n",
    "    if resp.status_code == 200:\n",
    "        return [np.asarray(data_object['embedding']) for data_object in resp.json()['data']]\n",
    "    else:\n",
    "        if (print_exceptions):\n",
    "            print(resp.json())\n",
    "        return [None for txt in input_text_list]   \n",
    "    \n",
    "# This method will return the cosine similarity of two ndarrays\n",
    "def cosine_sim(a, b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c9667-b52e-4a9c-bf38-08216c0f781e",
   "metadata": {},
   "source": [
    "# Generating Embeddings\n",
    "Now that I've got the methods in place to generate embeddings, I want to get embeddings for each of the reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77af0a01-612c-4d18-978d-8bf0ede295c1",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "I'll start by loading in all of the data into a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eb55c6-3753-497f-9af7-dc45c3dbfe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame containing all of the data scraped for each of the videos\n",
    "tnd_data_df_records = []\n",
    "for child_dir in tqdm(list(Path(\"data/theneedledrop_scraping/\").iterdir())):\n",
    "    \n",
    "    # Extract the video ID from the \n",
    "    cur_video_id = child_dir.name\n",
    "    \n",
    "    # Load in the details.json file\n",
    "    try:\n",
    "        with open(f\"data/theneedledrop_scraping/{cur_video_id}/details.json\", \"r\") as json_file:\n",
    "            cur_details_dict = json.load(json_file)\n",
    "    except:\n",
    "        cur_details_dict = {}\n",
    "        \n",
    "    # Load in the transcription.json file\n",
    "    try:\n",
    "        with open(f\"data/theneedledrop_scraping/{cur_video_id}/transcription.json\", \"r\") as json_file:\n",
    "            cur_transcription_dict = json.load(json_file)\n",
    "    except:\n",
    "        cur_transcription_dict = {}\n",
    "        \n",
    "    # Create a \"record\" for this video\n",
    "    tnd_data_df_records.append({\n",
    "        \"video_id\": cur_video_id,\n",
    "        \"details_dict\": cur_details_dict,\n",
    "        \"transcription_dict\": cur_transcription_dict\n",
    "    })\n",
    "    \n",
    "# Now, we want to create a DataFrame from the tnd_data_df_records\n",
    "tnd_data_df = pd.DataFrame.from_records(tnd_data_df_records)\n",
    "\n",
    "# Add a \"transcription string\" column \n",
    "tnd_data_df[\"transcription_str\"] = tnd_data_df[\"transcription_dict\"].apply(lambda x: x['text'] if 'text' in x else None)\n",
    "\n",
    "# Add a couple of columns indicating how long each of the transcriptions are \n",
    "tnd_data_df[\"transcription_length\"] = tnd_data_df[\"transcription_str\"].apply(lambda x: len(x) if x is not None else None)\n",
    "tnd_data_df[\"transcription_approx_tokens\"] = tnd_data_df[\"transcription_str\"].apply(lambda x: int(math.ceil(len(x)/3.5)) if x is not None else None)\n",
    "\n",
    "# Determining how many segments are in a transcription\n",
    "tnd_data_df[\"transcription_segment_amt\"] = tnd_data_df[\"transcription_dict\"].apply(lambda x: len(x['segments']) if 'segments' in x else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d678b90-9e77-4047-9696-01c0af3e4d32",
   "metadata": {},
   "source": [
    "### Simple Whole-Video Embeddings\n",
    "First order of business: getting embeddings for the videos who we believe are under a certain amount of tokens. According to [the OpenAI Tokenizer site](https://beta.openai.com/tokenizer), tokens are *roughly* ~4 characters. If I assume ~3.5 tokens/word instead (a more conservative estimate), I can calculate the approximate amount of tokens in each transcription. \n",
    "\n",
    "The [embeddings endpoint](https://beta.openai.com/docs/api-reference/embeddings/create) has a limit of 8192 tokens per text input. So, I'm going to try and get the embeddings for each transcription under ~7,500 tokens in length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21058fd0-c757-40a2-89a6-463d39a3671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the DataFrame to only show videos w/ less than 7,500 tokens in their transcription\n",
    "tnd_data_df_simple_token_subset = tnd_data_df.query(\"transcription_approx_tokens<=7500\").copy()\n",
    "\n",
    "# We're going to iterate through this DataFrame in chunks\n",
    "master_emb_df_list = []\n",
    "chunk_size = 20\n",
    "chunk_amt = int(math.ceil(len(tnd_data_df_simple_token_subset)/chunk_size))\n",
    "for cur_chunk in tqdm(list(range(chunk_amt))):\n",
    "    df_chunk = tnd_data_df_simple_token_subset[(cur_chunk*chunk_size):((cur_chunk+1)*chunk_size)]\n",
    "    transcription_list = [x.strip() for x in list(df_chunk[\"transcription_str\"])]\n",
    "    emb_list = generate_embeddings(transcription_list, print_exceptions=True)\n",
    "    \n",
    "    # Create a new DataFrame with these embeddings\n",
    "    cur_df_chunk_with_embeddings = df_chunk[[\"video_id\"]].copy()\n",
    "    cur_df_chunk_with_embeddings[\"whole_video_embedding\"] = emb_list\n",
    "    master_emb_df_list.append(cur_df_chunk_with_embeddings)\n",
    "    \n",
    "    # Determine how long we ought to sleep after this request\n",
    "    total_approx_tokens = sum(df_chunk[\"transcription_approx_tokens\"])\n",
    "    sleep_amt = int(math.ceil(((total_approx_tokens/150000)*60)*1.1))\n",
    "    sleep(sleep_amt)\n",
    "    \n",
    "# Now, make the master embedding DataFrame\n",
    "simple_token_subset_emb_df = pd.concat(master_emb_df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b886176-ddad-4191-838a-305ca7be210b",
   "metadata": {},
   "source": [
    "Now, with these embeddings in hand, I'm going to save them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220e7d7c-22a8-4209-ae7a-5bcff151a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving all of the whole-video embeddings \n",
    "for row in tqdm(list(simple_token_subset_emb_df.itertuples())):\n",
    "    video_folder = Path(f\"data/theneedledrop_scraping/{row.video_id}/\")\n",
    "    with open(f'{video_folder}/whole_video_embedding.json', \"w\") as json_file:\n",
    "        json.dump(row.whole_video_embedding.tolist(), json_file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c609d7d5-f2a5-4562-bd0b-a13d9d48a1b9",
   "metadata": {},
   "source": [
    "### Complex Whole-Video Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff103fc-2d20-42e1-bb67-6e59d1085d0a",
   "metadata": {},
   "source": [
    "### Video Sequence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459b45f2-92cf-4214-922a-f0ef49a59ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to step through each of the videos in our tnd_data_df and generate embeddings for \n",
    "# each \"segment chunk\".\n",
    "# video_segment_embedding_dict = {}\n",
    "for row in tqdm(list(tnd_data_df.itertuples())):\n",
    "    \n",
    "    # If the transcription dictionary isn't complete, skip it \n",
    "    if (\"segments\" not in row.transcription_dict):\n",
    "        continue\n",
    "        \n",
    "    # If the video's already had its segments embedded, skip it\n",
    "    if (row.video_id in video_segment_embedding_dict):\n",
    "        continue\n",
    "    \n",
    "    # Grab a list of the segments associated with this video\n",
    "    segment_list = row.transcription_dict[\"segments\"]\n",
    "    \n",
    "    # We're going to break up the segment list into chunks \n",
    "    segment_chunks_dict = {}\n",
    "    segments_per_chunk = 4\n",
    "    chunk_amt = int(math.ceil(len(segment_list)/segments_per_chunk))\n",
    "    for cur_chunk in range(chunk_amt):\n",
    "        segment_list_chunk = segment_list[(cur_chunk*segments_per_chunk):((cur_chunk+1)*segments_per_chunk)]\n",
    "        segment_id_range = (segment_list_chunk[0]['id'], segment_list_chunk[-1]['id'])\n",
    "        joined_segment_text = \" \".join([segment['text'].strip() for segment in segment_list_chunk])\n",
    "        segment_chunks_dict[segment_id_range] = joined_segment_text\n",
    "        \n",
    "    # Create a DataFrame out of the segment chunks we've found \n",
    "    segment_chunk_df = pd.DataFrame([{\"segment_range\": key, \"text\": val} for key, val in segment_chunks_dict.items()])\n",
    "\n",
    "    # Now, we're going to generate embeddings for each of these chunks, chunks at a time \n",
    "    chunk_size = 20\n",
    "    chunk_amt = int(math.ceil(len(segment_chunks_dict)/chunk_size))\n",
    "\n",
    "    # Iterate through in chunks\n",
    "    master_emb_df_list = []\n",
    "    for cur_chunk in range(chunk_amt):\n",
    "\n",
    "        # Get the subset of the DataFrame corresponding with this chunk \n",
    "        cur_chunk_df = segment_chunk_df[(cur_chunk*chunk_size):((cur_chunk+1)*chunk_size)].copy()\n",
    "        cur_chunk_text_list = [x.strip() for x in list(cur_chunk_df[\"text\"])]\n",
    "\n",
    "        # Grab the embeddings for this chunk list\n",
    "        emb_list = generate_embeddings(cur_chunk_text_list, print_exceptions=True)\n",
    "\n",
    "        # Add these embeddings to the cur_chunk_df\n",
    "        cur_chunk_df[\"embedding\"] = emb_list\n",
    "\n",
    "        # Add this \n",
    "        master_emb_df_list.append(cur_chunk_df)\n",
    "        \n",
    "        # Determine how long to sleep, and then sleep \n",
    "        total_approx_tokens = len(\" \".join(cur_chunk_text_list))/3.5\n",
    "        sleep_amt = int(math.ceil(((total_approx_tokens/150000)*60)*1.1)) + 5\n",
    "        sleep(sleep_amt)\n",
    "\n",
    "    # Concatenate all of the chunks together \n",
    "    cur_video_segment_embedding_df = pd.concat(master_emb_df_list)\n",
    "    video_segment_embedding_dict[row.video_id] = cur_video_segment_embedding_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d9149b-1eee-4fe6-af64-6db6e4546443",
   "metadata": {},
   "source": [
    "Now, we're going to save all of these embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261695af-d570-4676-8ecf-24031490e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_id, embedding_df in video_segment_embedding_dict.items():\n",
    "    if embedding_df[\"embedding\"] is None:\n",
    "        print(video_id)\n",
    "        continue\n",
    "    embedding_df[\"embedding\"] = embedding_df[\"embedding\"].apply(lambda x: x.tolist() if (not isinstance(x, list) and x is not None) else x)\n",
    "    \n",
    "    # Figure out where to save this \n",
    "    save_file_path = f\"data/theneedledrop_scraping/{video_id}/video_segment_embeddings.json\"\n",
    "    embedding_df.to_json(save_file_path, orient=\"records\", indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db593a0-5c6a-46b0-bf86-1e5cc35d0b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "70b6d7546de29863a58b9c57a86bc7d85299cc84f1fdf81f78934173ede7c021"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
