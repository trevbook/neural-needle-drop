{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e820dad-dea5-4dd0-a30e-04513457d15e",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "In another notebook (**TheNeedleDrop Scraping**), I defined a process to download data about each TheNeedleDrop review. Now, I want to create a couple of additional methods that create \"enriched\" versions of the files I've downloaded. \n",
    "\n",
    "Mostly, these \"enriched\" stats files will contain some important information about the video - description, publication date, album / artist information, etc. I'll try and parse the scores from the descriptions, and determine if videos are reviews or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88cde5f-da0e-4863-97b8-43e0056500d2",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will help to set up the rest of the notebook.\n",
    "\n",
    "First, I'll change my working directory to the repo's root. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a4f0d5-bf3b-4b5c-9e3a-e3ed9c503b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the cwd to the repo root\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06720f-11e2-4460-9566-d1421d2b2206",
   "metadata": {},
   "source": [
    "Next, I'll import some important modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1187cfea-66c6-442e-ad47-0b486761cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import plotly.express as px\n",
    "from time import sleep\n",
    "from Levenshtein import ratio\n",
    "import math\n",
    "\n",
    "# pytube-specific import statements\n",
    "from pytube import YouTube\n",
    "from pytube import Channel\n",
    "\n",
    "# Import statements\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c474f3a-52af-48d1-b12c-d0ce65d35ee3",
   "metadata": {},
   "source": [
    "We're also going to set up the Spotify client. We'll use this later on! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cd7cd-58e7-46c5-b9a7-0569ae87dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the API client\n",
    "spotify = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb8fb9-ada1-4801-8342-c30ca5f0e254",
   "metadata": {},
   "source": [
    "# Method Development\n",
    "I'm going to try and develop a number of \"helper methods\" that help to enrich the different files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bede49-bd95-4553-9c2d-ca19b0706066",
   "metadata": {},
   "source": [
    "### Detecting Reviews\n",
    "One method that I want to try and write: a \"review\" detector. Fantano releases a ton of different videos that aren't reviews - things like \"top ____ of the year\" sorta stuff.\n",
    "\n",
    "The method below will try to use a video's title / description to determine if a video is a review or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b08cd95-4945-4804-9787-12c62fee88b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will attempt to determine whether a video is a review or not \n",
    "def detect_review(video_details_dict):\n",
    "    \n",
    "    # Determine if \"album review\" is a substring of the video title, and return a bool indicating that\n",
    "    if video_details_dict[\"title\"].lower().find(\"album review\") == -1:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8652a58-d0f4-46af-9c3e-e4c1a7481360",
   "metadata": {},
   "source": [
    "### Classifying Videos\n",
    "Instead of just detecting if a video is an album review, we can write a more sophisticated \"video classifier\". This will search the titles to determine what type of video a particular video is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f1caf8-b711-48b0-a061-82865806496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will attempt to determine what type of video a particular video is \n",
    "def classify_video_type(video_details_dict):\n",
    "    \n",
    "    # Transform the video title to a lowercase string\n",
    "    lowercase_video_title = video_details_dict[\"title\"].lower()\n",
    "    \n",
    "    # Try and determine what type of video this one is by parsing the title \n",
    "    if lowercase_video_title.find(\"album review\") != -1 and lowercase_video_title.count(\"-\") > 0:\n",
    "        return \"album_review\"\n",
    "    elif lowercase_video_title.find(\"ep review\") != -1:\n",
    "        return \"ep_review\"\n",
    "    elif lowercase_video_title.find(\"track review\") != -1:\n",
    "        return \"track_review\"\n",
    "    elif lowercase_video_title.find(\"mixtape review\") != -1:\n",
    "        return \"mixtape_review\"\n",
    "    elif lowercase_video_title.find(\"yunoreview\") != -1 or lowercase_video_title.find(\"y u no review\") != -1:\n",
    "        return \"yunoreview\" \n",
    "    elif lowercase_video_title.find(\"weekly track roundup\") != -1 or lowercase_video_title.find(\"best & worst tracks\") != -1:\n",
    "        return \"weekly_track_roundup\"\n",
    "    elif lowercase_video_title.find(\"tnd podcast #\") != -1:\n",
    "        return \"tnd_podcast\"\n",
    "    elif lowercase_video_title.find(\"vinyl update\") != -1:\n",
    "        return \"vinyl_update\"\n",
    "    else:\n",
    "        return \"misc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b3179c-a129-4565-be27-211fd5e3d68e",
   "metadata": {},
   "source": [
    "### Extracting Review Scores\n",
    "The next method will attempt to use a regular expression to extract the review score from a video's description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84684ec1-fc29-42a8-b4e4-76fd7ab58ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will try to extract the review score from a video's description using a regex. \n",
    "# If successful, it'll return an int. If unsuccessful, it'll return None.\n",
    "def extract_review_score(video_details_dict):\n",
    "    \n",
    "    # Try to parse the review score from the video's description\n",
    "    try:\n",
    "        video_description = video_details_dict[\"shortDescription\"]\n",
    "        search = re.findall(r'[^0-9][0-9]{1,2}/10', video_description, re.IGNORECASE)\n",
    "        return int(search[-1].split(\"/\")[0])\n",
    "    \n",
    "    # Return None if we ran into an error\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3af7244-fd13-4dec-ae97-0190f2aca643",
   "metadata": {},
   "source": [
    "### Extracting Album Information\n",
    "For the videos that're identified as album reviews, we can determine the artist and album title that're associated with them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334a37b9-d226-495e-bfce-721e45d16da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will try and extract the album title and artist name from a review's title\n",
    "def extract_album_info(video_details_dict):\n",
    "    try: \n",
    "        video_title = video_details_dict['title'].lower()\n",
    "        single_dash = video_title.count(\"-\") == 1\n",
    "        if (single_dash):\n",
    "            artist, album_title = [x.strip() for x in video_title.split(\"album review\")[0].strip().split(\"-\")]\n",
    "        else:\n",
    "            video_title = video_title.split(\"album review\")[0]\n",
    "            artist, album_title = [x.strip() for x in re.split(r'\\s*-\\s*', video_title, maxsplit=1)]\n",
    "        return {\"artist\": artist, \"album_title\": album_title}\n",
    "    except:\n",
    "        return {\"artist\": None, \"album_title\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc9ade3-9e26-4969-8b70-958c6e2dcc64",
   "metadata": {},
   "source": [
    "### Searching Spotify for Album Info\n",
    "I can use the Spotify API (via [spotipy](https://spotipy.readthedocs.io/en/2.22.0/#getting-started)) to search for information about each of the albums. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1cd162-815c-446f-a3e4-a5984a8cf42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_spotify_album_id(album_title, artist):\n",
    "    \n",
    "    # Search Spotify for a particular album\n",
    "    try:\n",
    "        search_str = f\"{album_title} {artist}\".lower()\n",
    "        search_res = spotify.search(search_str, limit=1, type='album')\n",
    "        sleep(1)\n",
    "\n",
    "        # Extract some information from this Spotify search result\n",
    "        album_id = search_res[\"albums\"][\"items\"][0]['id']\n",
    "        spotify_res_artist = search_res[\"albums\"][\"items\"][0][\"artists\"][0][\"name\"]\n",
    "        spotify_res_album_title = search_res[\"albums\"][\"items\"][0][\"name\"]\n",
    "        spotify_res_search_str = f\"{spotify_res_album_title} {spotify_res_artist}\".lower()\n",
    "\n",
    "        # Determine how similar the result was to the search string \n",
    "        lev_sim = ratio(spotify_res_search_str.lower(), search_str)\n",
    "\n",
    "        # If the result is above a particular similarity, we're going to return that information\n",
    "        if (lev_sim >= 0.8):\n",
    "            return album_id\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    # If we run into an Exception, return None \n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f188b0eb-791e-4770-8d6b-e3a41dfd4825",
   "metadata": {},
   "source": [
    "In addition to getting the album IDs for these albums, we'll want a method to extract the album information from Spotify. The cells below will create those methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fefe752-2ae7-41a3-acf0-97e9efeb0ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will parse a Spotify album info dict\n",
    "def parseAlbumInfo(res):\n",
    "    \n",
    "    # We're going to store the results in this albumInfo dict\n",
    "    albumInfo = {}\n",
    "\n",
    "    # Indicate which fields we're looking to grab\n",
    "    fieldsToGrab = [\"album_type\", \"external_ids\", \"external_urls\", \"genres\", \"href\", \"id\", \"images\",\n",
    "                    \"label\", \"name\", \"popularity\", \"release_date\", \"release_date_precision\",\n",
    "                    \"total_tracks\", \"type\", \"uri\"]\n",
    "    for field in fieldsToGrab:\n",
    "        albumInfo[field] = res.get(field)\n",
    "\n",
    "    # Parse the tracks dict a little more\n",
    "    total_album_ms = 0\n",
    "    if \"tracks\" in res:\n",
    "\n",
    "        # Grab some fields about the track\n",
    "        albumInfo[\"track_count\"] = res[\"tracks\"].get(\"total\")\n",
    "\n",
    "        # Iterate through each of the tracks in the album and grab some information about each\n",
    "        trackList = []\n",
    "        for track in res[\"tracks\"][\"items\"]:\n",
    "            curTrackInfo = {}\n",
    "            track_fieldsToGrab = [\"duration_ms\", \"id\", \"name\"]\n",
    "            for trackField in track_fieldsToGrab:\n",
    "                curTrackInfo[f\"track_{trackField}\"] = track.get(trackField)\n",
    "\n",
    "                if (trackField == \"duration_ms\" and track.get(trackField) is not None):\n",
    "                    total_album_ms += track.get(trackField)\n",
    "\n",
    "            trackList.append(curTrackInfo)\n",
    "        albumInfo[\"tracks\"] = trackList\n",
    "        albumInfo[\"duration_ms\"] = total_album_ms\n",
    "\n",
    "    # Parse the artists dict a little more\n",
    "    if \"artists\" in res:\n",
    "        artistList = []\n",
    "        for artist in res[\"artists\"]:\n",
    "            artistInfo = {}\n",
    "            artist_fieldsToGrab = [\"href\", \"id\", \"name\", \"type\", \"uri\"]\n",
    "            for artistField in artist_fieldsToGrab:\n",
    "                artistInfo[artistField] = artist.get(artistField)\n",
    "            artistList.append(artistInfo)\n",
    "        albumInfo[\"artists\"] = artistList\n",
    "\n",
    "    # Parse the copyrights list a little more\n",
    "    if \"copyrights\" in res:\n",
    "        copyright_list = []\n",
    "        for copy in res[\"copyrights\"]:\n",
    "            copyInfo = {}\n",
    "            for key, val in copy.items():\n",
    "                copyInfo[f\"copyright_{key}\"] = val\n",
    "            copyright_list.append(copyInfo)\n",
    "        albumInfo[\"copyright\"] = copyright_list\n",
    "\n",
    "    # Return the albumInfo dict\n",
    "    return albumInfo\n",
    "\n",
    "# This method will extract information for multiple albums\n",
    "def albumInfoSpotify_multipleAlbums(albumID_list):\n",
    "\n",
    "    # Return the Spotify results\n",
    "    return spotify.albums(albumID_list)\n",
    "\n",
    "# This will return a list of the raw / parsed album data for a set of albums\n",
    "def spotify_albums(albumID_list):\n",
    "    \n",
    "    # Break this list up into chunks of 20 albums each\n",
    "    master_chunk_results = []\n",
    "    chunk_amt = math.ceil(len(albumID_list)/20)\n",
    "    for cur_chunk in tqdm(list(range(chunk_amt))):\n",
    "        list_chunk = albumID_list[(cur_chunk*20):((cur_chunk+1)*20)]\n",
    "        \n",
    "        # Parse the information for this chunk\n",
    "        albumInfo_list = albumInfoSpotify_multipleAlbums(list_chunk)\n",
    "        parsed_album_info = []\n",
    "        for albumInfo in albumInfo_list[\"albums\"]:\n",
    "            parsedInfo = parseAlbumInfo(albumInfo)\n",
    "            parsed_album_info.append({\"raw\": albumInfo, \"parsed\": parsedInfo})\n",
    "        master_chunk_results += parsed_album_info\n",
    "        sleep(5)\n",
    "    \n",
    "    # Return all of the albums we'd parsed\n",
    "    return master_chunk_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6342db6-e20e-46b0-967a-76b6d1a6bf28",
   "metadata": {},
   "source": [
    "### Searching Spotify for Artist Info\n",
    "I also want to search Spotify for the artist info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa35c71e-3e33-43b5-bedd-ee4028348fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_spotify_artist_id(artist):\n",
    "    \n",
    "    # Search Spotify for a particular album\n",
    "    try:\n",
    "        search_str = f\"{artist}\".lower()\n",
    "        search_res = spotify.search(search_str, limit=1, type='artist')\n",
    "        sleep(1)\n",
    "\n",
    "        # Extract some information from this Spotify search result\n",
    "        artist_id = search_res[\"artists\"][\"items\"][0][\"id\"]\n",
    "        spotify_res_artist = search_res[\"artists\"][\"items\"][0][\"name\"]\n",
    "        spotify_res_search_str = f\"{spotify_res_artist}\".lower()\n",
    "\n",
    "        # Determine how similar the result was to the search string \n",
    "        lev_sim = ratio(spotify_res_search_str.lower(), search_str)\n",
    "\n",
    "        # If the result is above a particular similarity, we're going to return that information\n",
    "        if (lev_sim >= 0.8):\n",
    "            return artist_id\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    # If we run into an Exception, return None \n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def artistInfoSpotify_multipleArtists(artistID_list):\n",
    "\n",
    "    # Return the Spotify results\n",
    "    return spotify.artists(artistID_list)\n",
    "\n",
    "def parseArtistInfo(res):\n",
    "    # We're going to store the results in this artistInfo dict\n",
    "    artistInfo = {}\n",
    "\n",
    "    # Indicate which fields we're looking to grab\n",
    "    fieldsToGrab = [\"genres\", \"href\", \"id\", \"images\", \"name\", \"popularity\", \"type\", \"uri\"]\n",
    "    for field in fieldsToGrab:\n",
    "        artistInfo[field] = res.get(field)\n",
    "\n",
    "    # Parse the external_urls dict a little more\n",
    "    if (\"external_urls\" in res):\n",
    "        for service, url in res[\"external_urls\"].items():\n",
    "            artistInfo[f\"{service}_url\"] = url\n",
    "\n",
    "    # Parse the followers dict a little more\n",
    "    if (\"followers\" in res):\n",
    "        for key, val in res[\"followers\"].items():\n",
    "            artistInfo[f\"followers_{key}\"] = val\n",
    "\n",
    "    return artistInfo\n",
    "\n",
    "# This will return raw/parsed artist data for a particular list of artists\n",
    "def spotify_artists(artistID_list):\n",
    "    \n",
    "    # Break up the list into chunks\n",
    "    master_chunk_results = []\n",
    "    chunk_amt = math.ceil(len(artistID_list)/50)\n",
    "    for cur_chunk in tqdm(list(range(chunk_amt))):\n",
    "        list_chunk = artistID_list[(cur_chunk*50):((cur_chunk+1)*50)]\n",
    "        \n",
    "        # Parse the information for this chunk\n",
    "        artistInfo_list = artistInfoSpotify_multipleArtists(list_chunk)\n",
    "        parsed_artist_info = []\n",
    "        for artistInfo in artistInfo_list[\"artists\"]:\n",
    "            parsedInfo = parseArtistInfo(artistInfo)\n",
    "            parsed_artist_info.append({\"raw\": artistInfo, \"parsed\": parsedInfo})\n",
    "        master_chunk_results += parsed_artist_info\n",
    "        sleep(5)\n",
    "    \n",
    "    # Return all of the artists we'd parsed\n",
    "    return master_chunk_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef39acd-c239-4cac-821c-26ff9d78ba27",
   "metadata": {},
   "source": [
    "# Testing Methods\n",
    "Now that I've defined a number of the methods above, I want to determine some information about the different videos I'd downloaded. \n",
    "\n",
    "I'll start by creating a DataFrame of all of the videos I'd downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddcb4e9-ebb1-4645-827f-b05117cf2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame containing all of the data scraped for each of the videos\n",
    "tnd_data_df_records = []\n",
    "for child_dir in tqdm(list(Path(\"data/theneedledrop_scraping/\").iterdir())):\n",
    "    \n",
    "    # Extract the video ID from the \n",
    "    cur_video_id = child_dir.name\n",
    "    \n",
    "    # Load in the details.json file\n",
    "    try:\n",
    "        with open(f\"data/theneedledrop_scraping/{cur_video_id}/details.json\", \"r\") as json_file:\n",
    "            cur_details_dict = json.load(json_file)\n",
    "    except:\n",
    "        cur_details_dict = {}\n",
    "        \n",
    "    # Load in the transcription.json file\n",
    "    try:\n",
    "        with open(f\"data/theneedledrop_scraping/{cur_video_id}/transcription.json\", \"r\") as json_file:\n",
    "            cur_transcription_dict = json.load(json_file)\n",
    "    except:\n",
    "        cur_transcription_dict = {}\n",
    "        \n",
    "    # Create a \"record\" for this video\n",
    "    tnd_data_df_records.append({\n",
    "        \"video_id\": cur_video_id,\n",
    "        \"details_dict\": cur_details_dict,\n",
    "        \"transcription_dict\": cur_transcription_dict\n",
    "    })\n",
    "    \n",
    "# Now, we want to create a DataFrame from the tnd_data_df_records\n",
    "tnd_data_df = pd.DataFrame.from_records(tnd_data_df_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f60dc11-56b0-4a1b-8eed-3d40acbe8def",
   "metadata": {},
   "source": [
    "With this collected, I'm going to start applying the different methods I've written to try and determine some info about the different videos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b1bf5-4ff8-4143-b77c-16c779b2d28e",
   "metadata": {},
   "source": [
    "### Detecting Reviews\n",
    "I've got two main questions for this section: \n",
    "\n",
    "1. What percentage of the videos are reviews? \n",
    "2. What are some of the titles that *aren't* reviews? (Knowing this could help with more \"enrichment\", since I'd be able to classify additional kinds of his videos.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017a721-676e-4ab8-acfc-d8307beb9c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to the tnd_data_df that indicates whether a video is a review or not \n",
    "tnd_data_df[\"is_review\"] = tnd_data_df[\"details_dict\"].apply(lambda x: detect_review(x))\n",
    "\n",
    "# Show value counts for the \"is_review\" column\n",
    "tnd_data_df[\"is_review\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628f09ee-aa7e-4e93-92bf-339affadd08e",
   "metadata": {},
   "source": [
    "What are some of the titles of videos that aren't reviews? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e64867a-8bea-412b-9de5-290ca5fc1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a couple of non-review video titles\n",
    "for row in tnd_data_df.query(\"is_review==False\")[\"details_dict\"].apply(\n",
    "    lambda x: x['title'] if 'title' in x else None).head(10):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff465447-fc9f-4487-b29e-6742b15b332f",
   "metadata": {},
   "source": [
    "### Classifying Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af20042-50a8-49d9-90e8-eb5feb37aa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnd_data_df[\"video_type\"] = tnd_data_df[\"details_dict\"].apply(\n",
    "    lambda x: classify_video_type(x))\n",
    "\n",
    "tnd_data_df[\"video_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd40e2c-4b61-40af-a56d-c802abde2353",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print a couple of non-review video titles\n",
    "for row in tnd_data_df.query(\"video_type=='misc'\")[\"details_dict\"].apply(\n",
    "    lambda x: x['title'] if 'title' in x else None).head(60):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61c4576-5ca6-45db-8d82-1748373b71f8",
   "metadata": {},
   "source": [
    "### Extracting Review Scores\n",
    "For those videos that *are* reviews: what's the score distribution? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435de153-be99-4974-bb5c-5b6665f74479",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame consisting of exclusively album review videos\n",
    "tnd_data_df_review_subset = tnd_data_df.query(\"video_type=='album_review'\").copy()\n",
    "tnd_data_df_review_subset[\"review_score\"] = tnd_data_df_review_subset[\"details_dict\"].apply(\n",
    "    lambda x: extract_review_score(x))\n",
    "\n",
    "# Create a visualization showing the distribution of scores \n",
    "score_value_count_df = tnd_data_df_review_subset[\"review_score\"].value_counts().reset_index().rename(\n",
    "    columns={\"index\": \"score\", \"review_score\": \"ct\"}).sort_values(\"score\", ascending=True).copy()\n",
    "fig = px.histogram(tnd_data_df_review_subset.query(\"review_score<=10\"), x=\"review_score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6293c92f-2c14-4b9e-bcec-756cde37fcd8",
   "metadata": {},
   "source": [
    "### Extracting Album Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37368b5e-b235-4b21-9024-3fe15152afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnd_data_df_review_subset[\"album_info_dict\"] = tnd_data_df_review_subset[\"details_dict\"].apply(\n",
    "    lambda x: extract_album_info(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4147556-4cf1-45c2-a4b2-2cda8b739ec6",
   "metadata": {},
   "source": [
    "### Searching Spotify for Album Info\n",
    "Next, I want to try and get all of the Spotify album IDs for each of the albums within the NeedleDrop data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f635dd51-d719-47df-a4b5-008f37b4c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_album_id_df_records = []\n",
    "for row in tqdm(list(tnd_data_df_review_subset.itertuples())):\n",
    "    spotify_album_id_df_records.append({\n",
    "        \"video_id\": row.video_id,\n",
    "        \"spotify_album_id\": search_spotify_album_id(row.album_info_dict[\"album_title\"], \n",
    "                                                    row.album_info_dict[\"artist\"])\n",
    "    })\n",
    "    sleep(2.5)\n",
    "spotify_album_id_df = pd.DataFrame.from_records(spotify_album_id_df_records)\n",
    "spotify_album_id_df.to_json(\"data/spotify_scraping/video_spotify_album_linkages.json\", indent=2, orient=\"records\")\n",
    "spotify_album_id_df = pd.read_json(\"data/spotify_scraping/video_spotify_album_linkages.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3796fb-5138-4599-9846-2f40861e006d",
   "metadata": {},
   "source": [
    "Now, quickly, I want to scrape Spotify for some album information. In theory, I'm going to do this in a much more sophisticated way once I have a full \"pipeline\" to collect this data, but for now, I'll just do it all at once.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04bce7a-6b1d-4b54-bcab-8bf84fc0c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "album_id_list = [x for x in list(spotify_album_id_df[\"spotify_album_id\"]) if x is not None]\n",
    "album_info_results = spotify_albums(album_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dbf63e-9f50-4ae1-bcb9-b116f8d844a4",
   "metadata": {},
   "source": [
    "Now, I'll save all of this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb6c2b-9203-48a6-a7e9-db035192a2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_album_data_path = Path(\"data/spotify_scraping/albums\")\n",
    "spotify_album_data_path.mkdir(exist_ok=True, parents=True)\n",
    "for album_info_res in tqdm(album_info_results):\n",
    "    cur_album_id = album_info_res[\"parsed\"][\"id\"]\n",
    "    with open(f\"{spotify_album_data_path}/{cur_album_id}.json\", \"w\") as json_file:\n",
    "        json.dump(album_info_res[\"parsed\"], json_file, indent=2, default=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5757e89-3574-443e-985f-101d62fdc515",
   "metadata": {},
   "source": [
    "As a part of saving all of this data, I'm going to save something of an \"artist index\" in the spotify scraping folder. This will ensure that I've got an easy map of `[album title, artist name]` --> `[album id]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31abfac8-4623-4ec4-80f7-0aa85418118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "album_index_df_records = []\n",
    "merged_album_df = spotify_album_id_df.merge(\n",
    "    tnd_data_df_review_subset, how=\"right\", on=\"video_id\").copy()\n",
    "for row in merged_album_df.itertuples():\n",
    "    if (row.spotify_album_id is not None):\n",
    "        album_search_str = f\"{row.album_info_dict['album_title']} {row.album_info_dict['artist']}\"\n",
    "        album_index_df_records.append({\n",
    "            \"album_title\": row.album_info_dict['album_title'],\n",
    "            \"artist\": row.album_info_dict['artist'],\n",
    "            \"album_search_str\": album_search_str,\n",
    "            \"spotify_album_id\": row.spotify_album_id})\n",
    "album_index_df = pd.DataFrame.from_records(album_index_df_records)\n",
    "album_index_df.to_json(\"data/spotify_scraping/album_index.json\", orient=\"records\", indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a4cadd-823c-4284-a59a-834101eb9844",
   "metadata": {},
   "source": [
    "### Searching Spotify for Artist Info\n",
    "Another thing I'm interested in doing: searching Spotify for the artist info for each of the different artists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8929cb-7883-4cfa-af0a-87ece56f723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_list = list(set([x for x in list(tnd_data_df_review_subset[\"album_info_dict\"].apply(lambda x: x['artist'] if 'artist' in x else None)) if x is not None]))\n",
    "spotify_artist_result_dict = {}\n",
    "for artist in tqdm(artist_list):\n",
    "    spotify_artist_result_dict[artist] = search_spotify_artist_id(artist)\n",
    "    sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea899794-fed1-4e4f-bce3-4054857fde9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an \"artist name\" column to the \n",
    "tnd_data_df_review_subset[\"artist_name\"] = tnd_data_df_review_subset[\"album_info_dict\"].apply(lambda x: x['artist'] if 'artist' in x else None)\n",
    "\n",
    "# Make a DataFrame out of the spotify_artist_result_dict\n",
    "spotify_artist_result_df = pd.DataFrame.from_records([{\"artist_name\": key, \"spotify_artist_id\": val} for key, val in spotify_artist_result_dict.items()])\n",
    "\n",
    "# Make a 'merged' version with the artist ID \n",
    "tnd_data_df_review_subset_merged = tnd_data_df_review_subset.merge(spotify_artist_result_df, how=\"left\", on=\"artist_name\").copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cf0310-e1c2-424c-9e88-f01866946b73",
   "metadata": {},
   "source": [
    "Next, we want to scrape Spotify for the artist information associated with each of the different artists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef98b6dd-2a10-43e4-8af9-35f06e8bd4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_artist_list = list(set([x for x in list(tnd_data_df_review_subset_merged[\"spotify_artist_id\"]) if x is not None]))\n",
    "all_artist_info = spotify_artists(all_artist_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74dae47-e2b0-4818-b683-360bfcfbb78a",
   "metadata": {},
   "source": [
    "Next, I'll save all of this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e133187-65dc-45eb-8ce6-c712e57aa647",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_artist_data_path = Path(\"data/spotify_scraping/artists\")\n",
    "spotify_artist_data_path.mkdir(exist_ok=True, parents=True)\n",
    "for artist_info_res in tqdm(all_artist_info):\n",
    "    cur_artist_id = artist_info_res[\"parsed\"][\"id\"]\n",
    "    with open(f\"{spotify_artist_data_path}/{cur_artist_id}.json\", \"w\") as json_file:\n",
    "        json.dump(artist_info_res[\"parsed\"], json_file, indent=2, default=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc00c10c-df72-4d74-90fe-68f0f8e4d771",
   "metadata": {},
   "source": [
    "Finally, I'll make something of an \"index\" for this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf74b30-67d6-4f17-9d76-125abc679cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_artist_index_df = tnd_data_df_review_subset_merged[\n",
    "    [\"artist_name\", \"spotify_artist_id\"]].drop_duplicates()\n",
    "spotify_artist_index_df = spotify_artist_index_df[spotify_artist_index_df[\"spotify_artist_id\"].notna()]\n",
    "spotify_artist_index_df.to_json(\"data/spotify_scraping/artist_index.json\", orient=\"records\", indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78fe08e-09df-43d4-b3f8-8072f08eda37",
   "metadata": {},
   "source": [
    "# Main Method\n",
    "Now that I've developed a number of \"enrichment\" methods, I'm going to develop one main one: a method that enriches a single video when provided with the `details.json` dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb69a14-aad6-43da-b4a8-1e6bb74b88b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're also going to assume that we've loaded in the Spotify indices\n",
    "spotify_indices = {}\n",
    "for index_type in [\"album\", \"artist\", \"song\"]:\n",
    "    index_path = Path(f\"data/spotify_scraping/{index_type}_index.json\")\n",
    "    if (index_path.exists()):\n",
    "        with open(index_path, \"r\") as json_file:\n",
    "            spotify_indices[index_type] = pd.DataFrame(json.load(json_file))\n",
    "            \n",
    "# Loading in the video_spotify_linkages\n",
    "video_spotify_linkages = {}\n",
    "for linkage_type in [\"album\", \"artist\", \"song\"]:\n",
    "    linkage_path = Path(f\"data/spotify_scraping/video_spotify_{linkage_type}_linkages.json\")\n",
    "    if (linkage_path.exists()):\n",
    "        with open(linkage_path, \"r\") as json_file:\n",
    "            video_spotify_linkages[linkage_type] = pd.DataFrame(json.load(json_file))\n",
    "\n",
    "# This method will enrich a video's details_dict\n",
    "def enrich_video_details(input_details_dict):\n",
    "    \n",
    "    # Set up the enriched_details_dict\n",
    "    enriched_details_dict = {}\n",
    "\n",
    "    # ===================================\n",
    "    # DUPLICATING THE DETAILS_DICT\n",
    "    # ===================================\n",
    "\n",
    "    # Add all of the keys from the input_details_dict to this new enriched_details_dict\n",
    "    for key, val in input_details_dict.items():\n",
    "        enriched_details_dict[key] = val\n",
    "\n",
    "    # ===================================\n",
    "    # INFERRING VIDEO INFORMATION\n",
    "    # ===================================\n",
    "\n",
    "    # First, we'll determine what kind of video this is\n",
    "    try: \n",
    "        inferred_video_type = classify_video_type(input_details_dict)\n",
    "        enriched_details_dict[\"inferred_video_type\"] = inferred_video_type\n",
    "    except:\n",
    "        enriched_details_dict[\"inferred_video_type\"] = None\n",
    "\n",
    "    # If the video is an album review, we're going to try and detect the score\n",
    "    try:\n",
    "        if (inferred_video_type == \"album_review\"):\n",
    "            inferred_review_score = extract_review_score(input_details_dict)\n",
    "            enriched_details_dict[\"inferred_review_score\"] = inferred_review_score\n",
    "    except:\n",
    "        enriched_details_dict[\"inferred_review_score\"] = None\n",
    "\n",
    "    # ===================================\n",
    "    # CREATING SPOTIFY LINKAGES\n",
    "    # ===================================\n",
    "\n",
    "    # Setting up the Spotify linkages dictionary\n",
    "    spotify_linkages_dict = {'album': [], 'artist': [], 'song': []}\n",
    "\n",
    "    # If the album is an album review, we're going to check if we've scraped the album data\n",
    "    if (enriched_details_dict[\"inferred_video_type\"] == \"album_review\"):\n",
    "\n",
    "        # Check if the album has had information scraped\n",
    "        video_linkage_df_query = video_spotify_linkages['album'].query(\"video_id==@cur_video_id\")\n",
    "        if (len(video_linkage_df_query) > 0):\n",
    "            spotify_linkages_dict['album'].append({\"review_album\": video_linkage_df_query.iloc[0]['spotify_album_id']})\n",
    "\n",
    "        # TODO - Instead of just checking the artist_index.json, we ought to try and \n",
    "        # actually *scrape* Spotify for data (if this is an unseen artist / album / song etc.)\n",
    "\n",
    "    # Now that we're finished adding the Spotify linkages, we can \n",
    "    enriched_details_dict[\"spotify_linkages\"] = spotify_linkages_dict\n",
    "    \n",
    "    # Return the enriched details\n",
    "    return enriched_details_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd24cbe0-afb9-4fba-a568-3b2e1dbb8d5b",
   "metadata": {},
   "source": [
    "### Enriching All Videos\n",
    "Now that I've written the details enrichment method, I should be able to enrich all of the videos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec6dba0-407d-40f8-8170-f14073a98598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all of the video folders\n",
    "for video_folder_path in tqdm(list(Path(\"data/theneedledrop_scraping/\").iterdir())):\n",
    "    if (video_folder_path.is_dir()):\n",
    "        cur_video_id = video_folder_path.stem\n",
    "        \n",
    "        # Load in the details dictionary \n",
    "        cur_video_folder_path = Path(f\"data/theneedledrop_scraping/{cur_video_id}/\")\n",
    "        cur_video_details_path = Path(f\"{cur_video_folder_path}/details.json\")\n",
    "        if (cur_video_details_path.exists()):\n",
    "            with open(cur_video_details_path, \"r\") as json_file:\n",
    "                cur_video_details_dict = json.load(json_file)\n",
    "        else:\n",
    "            cur_video_details_dict = None\n",
    "        \n",
    "        # If the details dictionary was successfully loaded, we can enrich it and save the result\n",
    "        if (cur_video_details_dict is not None):\n",
    "            \n",
    "            # Enrich the details dictionary\n",
    "            enriched_details_dict = enrich_video_details(cur_video_details_dict)\n",
    "            \n",
    "            # Now, save this enriched details dictionary\n",
    "            with open(f\"{cur_video_folder_path}/enriched_details.json\", \"w\") as json_file:\n",
    "                json.dump(enriched_details_dict, json_file, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "70b6d7546de29863a58b9c57a86bc7d85299cc84f1fdf81f78934173ede7c021"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
